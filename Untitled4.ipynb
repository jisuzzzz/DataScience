{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xZG9_AAeXEp3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "x_train = torch.FloatTensor([[1,2,1,1],[2,1,3,2],[3,1,3,4],[4,1,5,5],[1,7,5,5],[1,2,5,6],[1,6,6,6],[1,7,7,7]])\n",
        "y_train = torch.FloatTensor([[0,0,1],[0,0,1],[0,0,1],[0,1,0],[0,1,0],[0,1,0],[1,0,0],[1,0,0]])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w = torch.randn(4,3,requires_grad=True)\n",
        "b = torch.randn(1,3,requires_grad=True)\n",
        "optimizer = torch.optim.Adam([w,b],lr=0.1)"
      ],
      "metadata": {
        "id": "rIDdrDE_ZHjZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(3001):\n",
        "  h = torch.softmax(torch.mm(x_train, w)+b, dim=1)\n",
        "  cost = -torch.mean(torch.sum(y_train*torch.log(h),dim=1))\n",
        "  optimizer.zero_grad()\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "  with torch.no_grad():\n",
        "    if epoch % 300 == 0:\n",
        "      print(\"epoch: {}, cost: {:.6f}\".format(epoch, cost.item()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OjGF-1OZXfN",
        "outputId": "f12ed7df-2849-4e93-b2d5-bb4ada7e641e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, cost: 7.539644\n",
            "epoch: 300, cost: 0.199725\n",
            "epoch: 600, cost: 0.094651\n",
            "epoch: 900, cost: 0.054200\n",
            "epoch: 1200, cost: 0.034679\n",
            "epoch: 1500, cost: 0.023858\n",
            "epoch: 1800, cost: 0.017257\n",
            "epoch: 2100, cost: 0.012941\n",
            "epoch: 2400, cost: 0.009969\n",
            "epoch: 2700, cost: 0.007838\n",
            "epoch: 3000, cost: 0.006262\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = torch.FloatTensor([[1,11,10,9],[1,3,4,3],[1,1,0,1]])\n",
        "h_test = torch.softmax(torch.mm(x_test, w)+b, dim=1)\n",
        "print(h_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrhIELQPertO",
        "outputId": "93fe1842-edeb-47c6-f7e6-abeb2e96b9f6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000e+00, 2.0359e-16, 4.9599e-33],\n",
            "        [6.7932e-04, 6.9867e-01, 3.0066e-01],\n",
            "        [2.4414e-29, 2.5035e-10, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w.requires_grad_(False)\n",
        "b.requires_grad_(False)\n",
        "x_test = torch.FloatTensor([[1,11,10,9],[1,3,4,3],[1,1,0,1]])\n",
        "test_all = torch.softmax(torch.mm(x_test, w)+b, dim=1)\n",
        "print(test_all)\n",
        "print(torch.argmax(test_all, dim=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xwo14pbpadSR",
        "outputId": "41cbb8e9-4326-41c2-a986-f91160b3b661"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000e+00, 1.2777e-15, 2.6541e-32],\n",
            "        [4.8766e-03, 7.3287e-01, 2.6225e-01],\n",
            "        [2.5874e-28, 2.3760e-10, 1.0000e+00]])\n",
            "tensor([0, 1, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "x_train = torch.tensor([[1,2,1,1],[2,1,3,2],[3,1,3,4],[4,1,5,5],[1,7,5,5],[1,2,5,6],[1,6,6,6],[1,7,7,7]],dtype=torch.float)\n",
        "y_train = torch.tensor([2,2,2,1,1,1,0,0],dtype=torch.long)\n",
        "w = torch.randn(4,3,requires_grad=True)\n",
        "b = torch.randn(1,3,requires_grad=True)\n",
        "optimizer = torch.optim.Adam([w,b],lr=0.1)\n",
        "for epoch in range(3001):\n",
        "  h = torch.mm(x_train,w) + b\n",
        "  cost = F.cross_entropy(h, y_train)\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    if epoch % 300 == 0:\n",
        "      print(\"epoch: {}, cost: {:.6f}\".format(epoch, cost.item()))\n"
      ],
      "metadata": {
        "id": "YZN82EXwa2AH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2fda18b-7d26-457b-f8eb-991c7d9422ec"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, cost: 3.876289\n",
            "epoch: 300, cost: 0.152500\n",
            "epoch: 600, cost: 0.069838\n",
            "epoch: 900, cost: 0.039837\n",
            "epoch: 1200, cost: 0.025555\n",
            "epoch: 1500, cost: 0.017645\n",
            "epoch: 1800, cost: 0.012806\n",
            "epoch: 2100, cost: 0.009631\n",
            "epoch: 2400, cost: 0.007437\n",
            "epoch: 2700, cost: 0.005859\n",
            "epoch: 3000, cost: 0.004689\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "x_train = torch.tensor([[1,2,1,1],[2,1,3,2],[3,1,3,4],[4,1,5,5],[1,7,5,5],[1,2,5,6],[1,6,6,6],[1,7,7,7]],dtype=torch.float)\n",
        "y_train = torch.tensor([2,2,2,1,1,1,0,0],dtype=torch.long)\n",
        "# w = torch.randn(4,3,requires_grad=True)\n",
        "# b = torch.randn(1,3,requires_grad=True)\n",
        "model = nn.Linear(4,3)\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=0.1)\n",
        "for epoch in range(3001):\n",
        "  # h = torch.mm(x_train,w) + b\n",
        "  h = model(x_train)\n",
        "  cost = F.cross_entropy(h, y_train)\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    if epoch % 300 == 0:\n",
        "      print(\"epoch: {}, cost: {:.6f}\".format(epoch, cost.item()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6iarwkxhVid",
        "outputId": "2aff9667-73a2-4469-9249-a7a07a164c1c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, cost: 1.248351\n",
            "epoch: 300, cost: 0.092303\n",
            "epoch: 600, cost: 0.036317\n",
            "epoch: 900, cost: 0.019513\n",
            "epoch: 1200, cost: 0.012187\n",
            "epoch: 1500, cost: 0.008305\n",
            "epoch: 1800, cost: 0.005986\n",
            "epoch: 2100, cost: 0.004485\n",
            "epoch: 2400, cost: 0.003455\n",
            "epoch: 2700, cost: 0.002719\n",
            "epoch: 3000, cost: 0.002175\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "x_train = np.array([[1,2,1,1],[2,1,3,2],[3,1,3,4],[4,1,5,5],[1,7,5,5],[1,2,5,6],[1,6,6,6],[1,7,7,7]])\n",
        "y_train = np.array([2,2,2,1,1,1,0,0])\n",
        "logistic = LogisticRegression(penalty='none')\n",
        "logistic.fit(x_train, y_train)\n",
        "pred = logistic.predict([[1,11,10,9],[1,3,4,3],[1,1,0,1]])\n",
        "print(pred)"
      ],
      "metadata": {
        "id": "RZwVcaUBpKwP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b36b0e3-844a-4105-8b1c-19356ef6e92a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}